{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Chapter 1 of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.similar(\"monstrous\")\n",
    "text2.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.common_contexts([\"monstrous\", \"very\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(set(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(text3)) / len(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3.count(\"smote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 * text4.count(\"a\") / len(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "def percentage(count, total):\n",
    "    return 100 * count / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_diversity(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_diversity(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage(4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage(text4.count(\"a\"), len(text4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Rapper network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import requests\n",
    "import os\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rapper = pd.read_csv(\"../files/Rappers.csv\")\n",
    "\n",
    "WIKI_API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "params = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"exlimit\": \"1\",\n",
    "    \"explaintext\": \"1\",\n",
    "    \"prop\": \"extracts\",\n",
    "}\n",
    "\n",
    "def get_wiki_page(title):\n",
    "    params[\"titles\"] = title\n",
    "    response = requests.get(WIKI_API_URL, params=params)\n",
    "    return response.json()\n",
    "\n",
    "# Creation of the folder where we will store the pages' content\n",
    "if not os.path.exists(\"../files/rapper_pages\"):\n",
    "    os.makedirs(\"../files/rapper_pages\")\n",
    "\n",
    "for rapper in df_rapper[\"WikipediaPageName\"]:\n",
    "    rapper_page = get_wiki_page(rapper).get(\"query\").get(\"pages\").popitem()[1].get(\"extract\")\n",
    "    with open(f\"../files/rapper_pages/{rapper}.txt\", \"w\") as f:\n",
    "        f.write(rapper_page)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_list = os.listdir(\"../files/rapper_pages\")\n",
    "#Reorder the list of files in alphabetical order\n",
    "file_name_list.sort()\n",
    "\n",
    "#Sort df_rapper by WikipediaPageName\n",
    "df_rapper = df_rapper.sort_values(by=\"WikipediaPageName\")\n",
    "df_rapper = df_rapper.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new corpus with the rapper's pages\n",
    "corpus_root = \"../files/rapper_pages\"\n",
    "rapper_corpus = nltk.corpus.PlaintextCorpusReader(corpus_root, file_name_list)\n",
    "\n",
    "text = nltk.Text(rapper_corpus.words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.concordance(\"feat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.similar(\"feat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.common_contexts([\"feat\", \"featuring\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.dispersion_plot([\"featuring\", \"drug\", \"song\", \"album\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = list(nltk.bigrams(text))\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the words begining with \"h\"\n",
    "\n",
    "h_words = [w for w in set(text) if w.startswith(\"h\")]\n",
    "h_words.sort()\n",
    "\n",
    "#Print the first 5 words\n",
    "h_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.index(\"Snoop\")\n",
    "\n",
    "#Find the sentences where the word \"Snoop\" appears\n",
    "snoop_sentences = rapper_corpus.sents()\n",
    "snoop_sentences = [s for s in snoop_sentences if \"Snoop\" in s]\n",
    "snoop_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the five longest uppercased words\n",
    "\n",
    "upper_words = [w for w in set(text) if w.isupper()]\n",
    "upper_words.sort(key=len, reverse=True)\n",
    "upper_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tokens = len(text)\n",
    "nb_distinct_tokens = len(set(text))\n",
    "lexical_diversity = nb_distinct_tokens / nb_tokens\n",
    "\n",
    "#Compute the lexical diversity for the west and east coast rappers\n",
    "west_coast_rappers = df_rapper[df_rapper[\"Coast\"] == \"West\"]\n",
    "east_coast_rappers = df_rapper[df_rapper[\"Coast\"] == \"East\"]\n",
    "\n",
    "west_coast_rappers = west_coast_rappers[\"WikipediaPageName\"].tolist()\n",
    "east_coast_rappers = east_coast_rappers[\"WikipediaPageName\"].tolist()\n",
    "\n",
    "#We need to add \".txt\" to the rapper's name to match the file name\n",
    "west_coast_rappers = [rapper + \".txt\" for rapper in west_coast_rappers]\n",
    "east_coast_rappers = [rapper + \".txt\" for rapper in east_coast_rappers]\n",
    "\n",
    "west_coast_rappers_corpus = nltk.corpus.PlaintextCorpusReader(corpus_root, west_coast_rappers)\n",
    "east_coast_rappers_corpus = nltk.corpus.PlaintextCorpusReader(corpus_root, east_coast_rappers)\n",
    "\n",
    "west_coast_rappers_text = nltk.Text(west_coast_rappers_corpus.words())\n",
    "east_coast_rappers_text = nltk.Text(east_coast_rappers_corpus.words())\n",
    "\n",
    "west_coast_rappers_lexical_diversity = len(set(west_coast_rappers_text)) / len(west_coast_rappers_text)\n",
    "east_coast_rappers_lexical_diversity = len(set(east_coast_rappers_text)) / len(east_coast_rappers_text)\n",
    "\n",
    "print(f\"West coast rappers lexical diversity: {west_coast_rappers_lexical_diversity}\")\n",
    "print(f\"East coast rappers lexical diversity: {east_coast_rappers_lexical_diversity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a frequency distribution of the words in the text\n",
    "fdist = nltk.FreqDist(text)\n",
    "\n",
    "# Print the 75 most common words\n",
    "fdist.most_common(75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the four letter words\n",
    "four_letter_words = [w for w in set(text) if len(w) == 4]\n",
    "\n",
    "# Sort them by decreasing frequency\n",
    "four_letter_words.sort(key=fdist.get, reverse=True)\n",
    "four_letter_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum(len(w) for w in text)\n",
    "print(total)\n",
    "\n",
    "# Compute the average word length\n",
    "avg_word_length = total / len(text)\n",
    "print(avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_size(text):\n",
    "    return len(set(text))\n",
    "\n",
    "def percent(word, text):\n",
    "    return 100 * text.count(word) / len(text)\n",
    "\n",
    "print(vocab_size(text))\n",
    "print(percent(\"Dre\", text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Zipf's Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
